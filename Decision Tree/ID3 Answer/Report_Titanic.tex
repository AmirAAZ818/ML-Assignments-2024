\documentclass{article}
\linespread{1.25}

\usepackage[top = 1cm, right=2cm, left=2cm]{geometry}
\usepackage{sectsty}
\usepackage{amsmath}

\usepackage{graphicx}
\usepackage{xepersian}
\settextfont{B Nazanin}
\setlatinmonofont{CMU Serif}
%\setlatinmonofont{Times New Roman}
\setlatintextfont{Times New Roman}

% Set Latin Modern font for the bullets in itemizea
\newfontfamily\latinbullet{Latin Modern Roman}
\sectionfont{\fontsize{12}{15}\selectfont}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{پاسخ تکلیف 
	\lr{Decision Tree (ID3)}
}
\author{درس یادگیری ماشین}
\date{
	امیرحسین ابوالحسنی\\
	400405003
	}
	
	
% Commands
\newcommand{\column}[1]{\lr{\textit{#1}}}
\renewcommand{\labelitemi}{{\latinbullet\textbullet}} % Use the bullet from Latin Modern font
\newcommand{\tf}[1]{\text{\lr{#1}}}

\begin{document}
	\maketitle
	
	\begin{latin}
		\section{Suppose there is an attribute, "A," that consists of random values, and these
			values do not have any correlation with the class labels. Additionally, assume that
			"A" has a sufficient number of distinct values such that no two instances in the
			training dataset share the same value for "A." What would be the outcome if a
			decision tree is built using this attribute? What challenges or issues might arise in
			this scenario?}
	\end{latin}
	\noindent
	\subsection*{پاسخ}
	\subsubsection*{قسمت اول}
	با توجه به الگوریتم 
	\lr{ID3}،
	در ابتدا 
	\lr{information gain}
	ناشی از هر ویژگی را سنجیده و آن ویژگی که بیشترین 
	\lr{gain}
	را دارد انتخاب می‌کنیم\\
	(تعداد کلاس‌های هدف = \lr{k}):
	\[
	Gain(S, A) = Entropy(S) - \sum_{v \in A} \frac{| S_v |}{| S |}Entropy(S_v)
	\] 
	\[
	Entropy(S_v) = -\sum_{v} p_v\log (p_v) = -(P(S_v = 0) \log P(S_v = 0) + P(S_v = 1) \log P(S_v = 1) + \dots‌+ P(S_v = k) \log p(S_v = k))
	\]
	به علت یکتایی این ویژگی(کلید اصلی بودن) برای هر نمونه، همه ترم‌های $P(S_v = l) \log P(S_v = l)$ برابر با صفر می‌شود.  زیرا 
	\[
	P(S_v = l) = 0
	\]
	یا
	\[
	P(S_v = l) = 1
	\]
	 در نتیجه یکی از مضرب ها 0 خواهد شد و کل ترم‌ را 0 خواهد کرد. بدین صورت است که نتیجه می‌گیریم:
	\[
	Entropy(S_v) = 0
	\]
	و این ویژگی برای ریشه انتخاب می‌گردد:
	\[
	\argmax ~ \{Gain(S_v) |\forall A \in \text{\lr{Header}}\} = A
	\]
	در نتیجه این کار، ارتفاع درخت 1 شده و به تعداد مقادیر ویژگی 
	\lr{A}،
شاخه خواهیم داشت.
		\subsubsection*{قسمت دوم}
		در صورتی که این ویژگی با ویژگی هدف هیچ رابطه‌ای نداشته باشد، استفاده از این ویژگی کاملا اشتباه است و منجر به 
		\lr{overfit}
		می‌شود. زیرا عملا هیچ جایی برای 
		\lr{generalization}
		باقی نمی‌ماند.
	\begin{latin}
		\section{Answer the questions according to the following dataset:}
		\begin{center}
			\includegraphics{figs/Dataset_image.png}
		\end{center}
		
		\subsection{Create a decision tree model using the given dataset to predict the value
			of the final column, using all other columns as input features except for
			the first one(weekend). Clearly explain each step of the process, including
			your calculations, reasoning, and decisions made while constructing the
			tree. What is the model's overall classification accuracy?}
	\end{latin}
	1 - \lr{Root Node}
	\begin{latin}
		\textbf{Decision}
		\begin{center}
			\begin{tabular}{|c|c|c|c|c|}
				\hline
				Cinema & Tennis & Stay in & Shopping\\
				\hline
				\hline
				 6 & 2 & 1 & 1\\
				\hline
			\end{tabular}
		\end{center}
	\end{latin}
	\vspace{5pt}
	\[
	Entropy(S) = -\sum_{v \in S} p_v\log (p_v)
	\]
	\[
	Entropy(S) = -( 0.6 \times -0.73 + 0.2 \times -2.32‌ + 0.1 \times -3.32 + 0.1 \times -3.32) = 1.56
	\]
	\begin{latin}
		\textbf{Money}
		\begin{center}
			\begin{tabular}{|c|c|c|c|c|c|}
				\hline
				Value & Cinema & Tennis & Stay in & Shopping\\
				\hline
				\hline
				Rich & ‌3& 2 & 1 & 1\\
				\hline
				Poor & 3 & 0 & 0 & 0\\
				\hline
			\end{tabular}
		\end{center}
	\end{latin}
	\vspace{5pt}
	\[
	Entropy(S_{\text{\lr{v}}}) = -\sum_{v \in S} p_v\log (p_v)
	\]
	\[
	Entropy(S_{\tf{Rich}}) = -(0.42 \times -1.25 + 0.28 \times -1.83 + 0.14 \times -2.83 + 0.14 \times -2.83) = 1.82
	\]
	\[
	Entropy(S_{\tf{Poor}}) = -(0.42 \times -1.25) = 0.52
	\]
	\vspace{10pt}
	\[
	Gain(S, \tf{Money}) = Entropy(S) - \sum_{v \in \tf{Money}} \frac{| S_v |}{| S |}Entropy(S_v)
	\] 
	\[
	\sum_{v \in \tf{Money}} \frac{| S_v |}{| S |}Entropy(S_v) = \frac{7}{10} \times 1.82 + \frac{3}{10} \times 0.52 = 1.43
	\]
	\[
	Gain(S, \tf{Money}) = 1.56 - 1.43 = 0.13
	\]
	\begin{latin}
		\textbf{Parents}
		\begin{center}
			\begin{tabular}{|c|c|c|c|c|c|}
				\hline
				Value & Cinema & Tennis & Stay in & Shopping\\
				\hline
				\hline
				Yes & ‌5& 0 & 0 & 0\\
				\hline
				No & 1 & 2 & 1 & 1\\
				\hline
			\end{tabular}
		\end{center}
	\end{latin}
	\vspace{5pt}
	\[
	Entropy(S_{\tf{v}}) = -\sum_{v \in S} p_v\log (p_v)
	\]
	\[
	Entropy(S_{\tf{Yes}}) = -(\frac{5}{5} \times 0) = 0
	\]
	\[
	Entropy(S_{\tf{No}}) = -(0.2 \times -2.32 + 0.4 \times -1.32 + 0.2 \times -2.32 + 0.2 \times -2.32) = 1.92
	\]
	\vspace{10pt}
	\[
	Gain(S, \tf{Parents}) = Entropy(S) - \sum_{v \in \tf{Parents}} \frac{| S_v |}{| S |}Entropy(S_v)
	\] 
	\[
	\sum_{v \in \tf{Parents}} \frac{| S_v |}{| S |}Entropy(S_v) = \frac{5}{10} \times 0 + \frac{5}{10} \times 1.92 = 0.96
	\]
	\[
	Gain(S, \tf{Parents}) = 1.56 - 0.96 = 0.6
	\]
	\begin{latin}
		\textbf{Weather}
		\begin{center}
			\begin{tabular}{|c|c|c|c|c|c|}
				\hline
				Value & Cinema & Tennis & Stay in & Shopping\\
				\hline
				\hline
				Sunny & ‌1& 2 & 0 & 0\\
				\hline
				Windy & 3 & 0 & 0 & 1\\
				\hline
				Rainy & 2 & 0 & 1 & 0\\
				\hline
			\end{tabular}
		\end{center}
	\end{latin}
	\vspace{5pt}
	\[
	Entropy(S_{\tf{v}}) = -\sum_{v \in S} p_v\log (p_v)
	\]
	\[
	Entropy(S_{\tf{Sunny}}) = -(\frac{1}{3} \times -1.59 + \frac{2}{3} \times -0.59) = 0.92
	\]
	\[
	Entropy(S_{\tf{Windy}}) = -(0.75 \times -0.41 + 0.25 \times -2) = 0.8
	\]
	\[
	Entropy(S_{\tf{Rainy}}) = -(\frac{2}{3} \times -0.59 + \frac{1}{3} \times -1.59) = 0.92
	\]
	\vspace{10pt}
	\[
	Gain(S, \tf{Weather}) = Entropy(S) - \sum_{v \in \tf{Weather}} \frac{| S_v |}{| S |}Entropy(S_v)
	\] 
	\[
	\sum_{v \in \tf{Weather}} \frac{| S_v |}{| S |}Entropy(S_v) = \frac{3}{10} \times 0.92 + \frac{4}{10} \times 0.8 + \frac{3}{10} \times 0.92 = 0.87
	\]
	\[
	Gain(S, \tf{Weather}) = 1.56 - 0.87 = 0.69
	\]
	\newpage
	\begin{latin}
		\textbf{Picking The Best Attribute}
		\begin{center}
			\begin{tabular}{|c|c|}
				\hline
				Attribute & Information Gain\\
				\hline
				\hline
				Money & 0.13\\
				Parents & 0.6\\
				Weather & 0.69\\
				\hline
			\end{tabular}
		\end{center}
	\end{latin}
	ویژگی انتخابی،
	\lr{Weather}
	می‌باشد.\\
	2 - \lr{Sunny Node}
	\begin{latin}
		\textbf{Decision}
		\begin{center}
			\begin{tabular}{|c|c|c|c|c|}
				\hline
				Cinema & Tennis & Stay in & Shopping\\
				\hline
				\hline
				1 & 2 & 0 & 0\\
				\hline
			\end{tabular}
		\end{center}
		For $W_1, W_2, W_{10}$
	\end{latin}
	\vspace{5pt}
	\[
	Entropy(S) = -\sum_{v \in S} p_v\log (p_v)
	\]
	\[
	Entropy(S) = -( 0.33 \times -1.59 + 0.66 \times -0.59) = 0.91
	\]
	\begin{latin}
		\textbf{Money}
		\begin{center}
			\begin{tabular}{|c|c|c|c|c|c|}
				\hline
				Value & Cinema & Tennis & Stay in & Shopping\\
				\hline
				\hline
				Rich & ‌3& 2 & 1 & 1\\
				\hline
				Poor & 3 & 0 & 0 & 0\\
				\hline
			\end{tabular}
		\end{center}
	\end{latin}
	\vspace{5pt}
	\[
	Entropy(S_{\text{\lr{v}}}) = -\sum_{v \in S} p_v\log (p_v)
	\]
	\[
	Entropy(S_{\tf{Rich}}) = -(0.42 \times -1.25 + 0.28 \times -1.83 + 0.14 \times -2.83 + 0.14 \times -2.83) = 1.82
	\]
	\[
	Entropy(S_{\tf{Poor}}) = -(0.42 \times -1.25) = 0.52
	\]
	\vspace{10pt}
	\[
	Gain(S, \tf{Money}) = Entropy(S) - \sum_{v \in \tf{Money}} \frac{| S_v |}{| S |}Entropy(S_v)
	\] 
	\[
	\sum_{v \in \tf{Money}} \frac{| S_v |}{| S |}Entropy(S_v) = \frac{7}{10} \times 1.82 + \frac{3}{10} \times 0.52 = 1.43
	\]
	\[
	Gain(S, \tf{Money}) = 1.56 - 1.43 = 0.13
	\]
	\begin{latin}
		\textbf{Parents}
		\begin{center}
			\begin{tabular}{|c|c|c|c|c|c|}
				\hline
				Value & Cinema & Tennis & Stay in & Shopping\\
				\hline
				\hline
				Yes & ‌5& 0 & 0 & 0\\
				\hline
				No & 1 & 2 & 1 & 1\\
				\hline
			\end{tabular}
		\end{center}
	\end{latin}
	\vspace{5pt}
	\[
	Entropy(S_{\tf{v}}) = -\sum_{v \in S} p_v\log (p_v)
	\]
	\[
	Entropy(S_{\tf{Yes}}) = -(\frac{5}{5} \times 0) = 0
	\]
	\[
	Entropy(S_{\tf{No}}) = -(0.2 \times -2.32 + 0.4 \times -1.32 + 0.2 \times -2.32 + 0.2 \times -2.32) = 1.92
	\]
	\vspace{10pt}
	\[
	Gain(S, \tf{Parents}) = Entropy(S) - \sum_{v \in \tf{Parents}} \frac{| S_v |}{| S |}Entropy(S_v)
	\] 
	\[
	\sum_{v \in \tf{Parents}} \frac{| S_v |}{| S |}Entropy(S_v) = \frac{5}{10} \times 0 + \frac{5}{10} \times 1.92 = 0.96
	\]
	\[
	Gain(S, \tf{Parents}) = 1.56 - 0.96 = 0.6
	\]
	\begin{latin}
		\textbf{Weather}
		\begin{center}
			\begin{tabular}{|c|c|c|c|c|c|}
				\hline
				Value & Cinema & Tennis & Stay in & Shopping\\
				\hline
				\hline
				Sunny & ‌1& 2 & 0 & 0\\
				\hline
				Windy & 3 & 0 & 0 & 1\\
				\hline
				Rainy & 2 & 0 & 1 & 0\\
				\hline
			\end{tabular}
		\end{center}
	\end{latin}
	\vspace{5pt}
	\[
	Entropy(S_{\tf{v}}) = -\sum_{v \in S} p_v\log (p_v)
	\]
	\[
	Entropy(S_{\tf{Sunny}}) = -(\frac{1}{3} \times -1.59 + \frac{2}{3} \times -0.59) = 0.92
	\]
	\[
	Entropy(S_{\tf{Windy}}) = -(0.75 \times -0.41 + 0.25 \times -2) = 0.8
	\]
	\[
	Entropy(S_{\tf{Rainy}}) = -(\frac{2}{3} \times -0.59 + \frac{1}{3} \times -1.59) = 0.92
	\]
	\vspace{10pt}
	\[
	Gain(S, \tf{Weather}) = Entropy(S) - \sum_{v \in \tf{Weather}} \frac{| S_v |}{| S |}Entropy(S_v)
	\] 
	\[
	\sum_{v \in \tf{Weather}} \frac{| S_v |}{| S |}Entropy(S_v) = \frac{3}{10} \times 0.92 + \frac{4}{10} \times 0.8 + \frac{3}{10} \times 0.92 = 0.87
	\]
	\[
	Gain(S, \tf{Weather}) = 1.56 - 0.87 = 0.69
	\]
	\newpage
	\begin{latin}
		\textbf{Picking The Best Attribute}
		\begin{center}
			\begin{tabular}{|c|c|}
				\hline
				Attribute & Information Gain\\
				\hline
				\hline
				Money & 0.13\\
				Parents & 0.6\\
				Weather & 0.69\\
				\hline
			\end{tabular}
		\end{center}
	\end{latin}
	ویژگی انتخابی،
	\lr{Weather}
	می‌باشد.
\end{document}