\documentclass{article}
\linespread{1.25}

\usepackage[top = 2cm, right=2cm, left=2cm]{geometry}
\usepackage{graphicx}
\usepackage[section]{placeins}
\usepackage[hidelinks, urlcolor=blue]{hyperref}
\usepackage{float} % for image position in exatly where you want
\usepackage[perpage, stable]{footmisc}
\usepackage{amsmath}
\usepackage{titling}

\usepackage{xepersian}
\settextfont{B Nazanin}
\setlatinmonofont{CMU Serif}
%\setlatinmonofont{Times New Roman}
\setlatintextfont{Times New Roman}

% Set Latin Modern font for the bullets in itemizea
\newfontfamily\latinbullet{Latin Modern Roman}





% Commands
\newcommand{\column}[1]{\lr{\textit{#1}}}
\renewcommand{\labelitemi}{{\latinbullet\textbullet}} % Use the bullet from Latin Modern font

% Custom title page setup
\makeatletter
\def\maketitle{
	\begin{titlepage}
		\begin{center}
			\vspace*{2cm}
			
			{\Large\bfseries درس یادگیری ماشین\par}
			\vspace{2cm}
			
			{\Huge\bfseries گزارش تکلیف
				\lr{Linear Regression}\par}
			\vspace{3cm}
			
			{\large\bfseries استاد درس:\par}
			{\large دکتر افتخاری\par}
			\vspace{1.5cm}
			
			{\large\bfseries نگارش:\par}
			{\large امیرحسین ابوالحسنی\par}
			{\large شماره دانشجویی: 400405003\par}
			\vspace{2cm}
			
			\vfill  % pushes the date to bottom
			
			{\large\bfseries پاییز \lr{1403}}
			
		\end{center}
	\end{titlepage}
	\setcounter{page}{1}
}
\makeatother


\begin{document}
	\maketitle	
	\tableofcontents
	\newpage
	\section{مقدمه
		\footnote{مقدمه با مدل \lr{GPT 3} نوشته شده است.}
	}
	
	رگرسیون خطی یکی از ساده‌ترین و پرکاربردترین روش‌ها در یادگیری نظارت‌شده برای مدل‌سازی رابطه بین یک متغیر وابسته و یک یا چند متغیر مستقل است. با فرض وجود یک رابطه خطی بین ویژگی‌های ورودی و متغیر هدف، رگرسیون خطی تلاش می‌کند بهترین خط را پیدا کند که خطای پیش‌بینی را به حداقل برساند.
	
	هنگامی که تنها یک متغیر مستقل وجود دارد، این روش به عنوان رگرسیون خطی ساده شناخته می‌شود. با این حال، مشکلات دنیای واقعی اغلب شامل چندین عامل مؤثر بر متغیر هدف است. در چنین مواردی از رگرسیون خطی چندگانه استفاده می‌شود، جایی که مدل چندین متغیر مستقل را برای پیش‌بینی متغیر وابسته در نظر می‌گیرد. به صورت ریاضی، این رابطه به صورت زیر مدل‌سازی می‌شود:
	
	\[
	y = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_n x_n + \epsilon
	\]
	
	در اینجا \( y \) مقدار پیش‌بینی‌شده، \( w_0 \) عرض از مبدأ، \( w_1, w_2, \dots, w_n \) ضرایب متغیرهای مستقل \( x_1, x_2, \dots, x_n \)، و \( \epsilon \) خطای مدل است.
	
	در عمل، رگرسیون خطی می‌تواند با استفاده از تکنیک‌های مختلفی حل شود، مانند حل بسته
	\footnote{\lr{Closed Form Solution}}
	که از کمینه‌سازی میانگین مربعات خطا 
	\footnote{\lr{Mean Squared Error (MSE)}}
	 مشتق شده است یا الگوریتم‌های بهینه‌سازی مانند گرادیان نزولی. علاوه بر این، تکنیک‌های تنظیم 
	 \footnote{\lr{Regularization}}
	  مانند تنظیم \( L_2 \) (رگرسیون ریدج) می‌توانند برای جلوگیری از بیش‌برازش با جریمه کردن مقادیر بزرگ ضرایب اعمال شوند. این روش‌ها به‌ویژه در مقابله با چندخطی بودن یا داده‌های پرنویز، پایداری مدل را تضمین می‌کنند.
	
	این گزارش به بررسی رگرسیون خطی ساده و چندگانه می‌پردازد و بر حل بسته در حالت معمولی و با تنظیم $L_2$، بهینه‌سازی گرادیان نزولی در دو حالت گرادیان نزولی تصادفی
	\footnote{\lr{Stochastic Gradient Descent}}
	 و دسته‌ای
	 \footnote{\lr{Batch Gradient Descent}}
	 تاکید دارد.
	  . همچنین عملکرد مدل در روش‌های مختلف با استفاده از داده‌های آموزش و آزمایش مقایسه شده است.
	\section{رگرسیون خطی تک متغیره}
	\subsection{تابع هزینه رگرسیون خطی چیست؟}
	تابع هزینه رگرسیون خطی با نام میانگین مربع خطاها یا 
	\lr{Mean Square Error (MSE)}
	شناخته می‌شود.
	\[J(\theta) = \frac{1}{n}\sum_{i = 1}^n (y^i - \hat{y}^i)^2\]
		
	\subsection{آموزش رگرسیون خطی}
	\subsubsection{راه حل بسته}
	در حل بسته، باید رابطه زیر حل شود تا $\theta$ پیدا شود.
	$$\theta = (X^TX)^{-1}X^T\vec{y}$$
	البته ممکن است
	$X^TX$
	معکوس پذیر نباشد (که در تکلیف همین مورد برای داده‌های \lr{Train} اتفاق می‌افتد.) که در این صورت می توان از \lr{Moore-Penrose pseudo-inverse}
	در رابطه راه حل بسته استفاده کرد:
	$$\theta = (X^TX)^{+}X^T\vec{y}$$
	پاسخ به دست آمده:
	$$
	y = \theta_0 + \theta_1x
	$$
	$$
	\theta_0 = -3.8957 , \theta_1 = 1.1930
	$$
	\newpage
	\subsubsection{گرادیان کاهشی تصادفی}
	در این روش از الگوریتم گرادیان کاهشی تصادفی برای بهینه سازی تابع هزینه استفاده می‌شود.\\
	پاسخ به دست آمده:
	$$
	y = \theta_0 + \theta_1x
	$$
	$$
	\theta_0 = -3.8481, \theta_1 = 1.0570
	$$
	\subsubsection{گرادیان کاهشی دسته‌ای}
	در این روش از الگوریتم گرادیان کاهشی دسته‌ای برای بهینه سازی تابع هزینه استفاده می‌شود.\\
	پاسخ به دست آمده:
	$$
	y = \theta_0 + \theta_1x
	$$
	$$
	\theta_0 = -3.5858, \theta_1 = 1.1619
	$$
	\subsection{مصور سازی مدل با داده‌ها}
	هر سه پاسخ به دست آمده در کنار داده‌ها در شکل 
	\ref{fig: model with data}
	 به تصویر کشیده شده است.‌ همانطور که دیده می شود گرادیان کاهشی دسته‌ای بهتر از تصادفی عمل کرده است.
	\begin{figure}[H]
	 	\centering
	 	\includegraphics[scale=0.5]{figs/model_with_data}
	 	\caption{خطوط به دست آمده از سه روش مختلف}
	 	\label{fig: model with data}
	\end{figure}
	 
	\subsection{مقایسه پیش‌بینی مدل‌ها}
	پس از آموزش هر مدل، برای تست کردن، به آن‌ها داده‌های 
	\[X = 6.2, 12.8, 22.1, 30\]
	داده می‌شود. نزدیکی خروجی هر مدل، به مدلی که از راه حل بسته به دست آمده است می‌تواند مشخص کند آن شیوه بهینه سازی چقدر خوب توانسته پارامترهایی را پیدا کند که تابع هزینه را کمینه می‌کنند.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{figs/prediction_one_var}
		\caption{خروجی مدل در روش}
		\label{fig: prediction one var}
	\end{figure}
	
		 
	\subsection{مقایسه مدل‌های آموزش دیده}
	از آنحایی که پاسخ بسته بهترین پاسخی است که می‌توان به آن رسید، مقایسه هر خطی با پاسخ بسته می‌تواند نشان بدهد آن مدل چقدر خوب است.(شکل 
	\ref{fig: lines one var}
	)
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{figs/lines_one_var}
		\caption{خروجی مدل در روش}
		\label{fig: lines one var}
	\end{figure}
	
	\subsection{بررسی رفتار تابع هزینه}\label{resuling}
	از مهمترین روش‌های مانیتور کردن کیفیت یادگیری یک مدل، بررسی رفتار تابع هزینه در هنگام آموزش می‌باشد.\\
	همانطور که در شکل
	\ref{fig: cost one var}
	دیده می شود، گرادیان تصادفی بعد از 20 تکرار دیگر نمی‌تواند تابع هزینه را مینیمم کند و احتمالا در یک بهینه محلی گیرکرده است. از طرفی گرادیان کاهشی دسته‌ای توانسته به نزدیکی مقدار مینیمم کلی تابع هزینه برسد.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{figs/costvsepoch_one_var}
		\caption{نمودار مقدار تابع هزینه با توجه به هر تکرار در زمان یادگیری}
		\label{fig: cost one var}
	\end{figure}
	\subsection{کدام روش بهینه سازی ترجیج داده می‌شود؟}
	با توجه به بخش
	\ref{resuling}	
	و شکل 
	\ref{fig: cost one var}
	می‌توان گفت از آنجایی که گرادیان کاهشی دسته‌ای نسبت به تصادفی بیشتر به کل داده‌ها دید دارد، بهتر می‌تواند مسیر خود را به سمت مینیمم تابع هزینه پیدا کند، مخصوصا زمانی که داده‌ها کم هستند و میتوانند در رم باشند.
	
	\section{رگرسیون چند متغیره}
	\subsection{پیش پردازش‌ داده‌ها}
		\subsubsection{انکود داده‌ها}
		بعضی از متغیر‌های دیتاست کتگوریکال هستند و نیاز بود که اینها متغیر‌ها انکود شوند.
		از \lr{One Hot Encoding (OHE)}
		و 
		\lr{Integer Encoding (IE)}
		برای انکودینگ استفاده شد. متغیر‌
		\column{region}
	با استفاده از \lr{OHE} انکود شده و برای ویژگی‌های 
	\column{smoker, gender}
	از روش دوم انکودینگ استفاده شده است.\\
		\textbf{سوال: چرا برای این سه ویژگی از روش ‌های متفاوت استفاده شد؟}\\
	زمانی از 
	\lr{IE}
	برای انکودینگ استفاده می‌کنیم که بدانیم ترتیب در مقادیر ویژگی مهم است. ممکن است این سوال پیش بیاید که پس چرا برای ویژگی 
	\column{gender}
	از این روش استفاده می‌شود. می‌توان گفت عملا تفاوتی نمی‌کند چون تنها دو مقدار برای این ویژگی وجود دارد.
	و اما زمانی که از روش 
	\lr{OHE}
	استفاده می‌کنیم دیگر ترتیب اثری ندارد و با هر ویژگی ایجاد شده که مقدار 0 یا 1 را دارد به طور عادلانه برخورد می‌شود،‌ همانطور که از ماهیت ویژگی
	\column{region}
	مشخص است.
	
	\subsubsection{استانداردسازی}
	در زمان انجام تکلیف، دیده شد که عدم استانداردسازی داده‌ها باعث انفجار گرادیان و پارامترها می‌شود.
	به همین علت استانداردسازی روی داده‌ها های آموزشی انجام شد و با همان پارامتر‌ها، داده های تست نیز استانداردسازی شدند.
	
	
	\subsection{آموزش مدل}
	\subsubsection{راه حل بسته}
	در این بخش، چون نمی‌توان از $X^TX$ (که $X$ ماتریس داده آموزشی باشد) معکوس گرفت، از
	\lr{Moore-Penrose pseudo inverse}
	استفاده می‌کنیم.\\
	همچنین با توجه به نمودار
	\ref{fig: size closed}
	می‌توان مشاهده کرد که عملکرد راه حل بسته با مقدار مختلف از داده‌های یادگیری، در ابتدا بسیار کم بوده اما بعد از حدود 20 داده توانسته به خطای ثابتی برسد.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{figs/closed_size_test}
		\caption{نمودار نشان‌دهنده مقدار میانگین خطا نسبت به اندازه دیتاست آموزش}
		\label{fig: size closed}
	\end{figure}
	\subsubsection{گرادیان کاهشی تصادفی}
	مدل رگرسیون خطی را با گرادیان کاهشی تصادفی اموزش میدهیم. نمودار
	\ref{fig: sgd_costfull}
	نشان دهنده روند 
	\lr{Test Loss}
	و
	\lr{Train Loss}
	را برای یادگیری روی تمام داده‌های بخش
	\lr{Train}
	نشان می‌دهد.
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{figs/sgd_costvsiter_2reg}
		\caption{نمودار رفتار خطا تست و خطا آموزش برای گرادیان کاهشی تصادفی (کمترین خطا در تکرار 41 با مقدار 17.85 می‌باشد.)}
		\label{fig: sgd_costfull}
	\end{figure}
	همچنین میانگین خطا نسبت به اندازه دیتاست در شکل 
	\ref{fig: sgd datasize}
	بررسی شده است.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{figs/sgd_mean_test_error}
		\caption{میانگین خطا مدل آموزش داده شده روی اندازه‌های مختلف دیتاست}
		\label{fig: sgd datasize}
	\end{figure}
	\subsubsection{گرادیان کاهشی دسته‌ای}
	مدل رگرسیون خطی را با گرادیان کاهشی دسته‌ای اموزش میدهیم. نمودار
	\ref{fig: gd_costfull}
	نشان دهنده روند 
	\lr{Test Loss}
	و
	\lr{Train Loss}
	را برای یادگیری روی تمام داده‌های بخش
	\lr{Train}
	نشان می‌دهد.
	
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{figs/gd_costvsiter_2}
		\caption{نمودار رفتار هزینه تست و هزینه آموزش برای گرادیان کاهشی دسته‌ای (کمترین خطا در تکرار 15 با مقدار 17.85 می‌باشد.)}
		\label{fig: gd_costfull}
	\end{figure}
	همچنین میانگین خطا نسبت به اندازه دیتاست در شکل 
	\ref{fig: gd datasize}
	بررسی شده است.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{figs/gd_mean_test_error}
		\caption{میانگین خطا مدل آموزش داده شده روی اندازه‌های مختلف دیتاست}
		\label{fig: gd datasize}
	\end{figure}
	\subsubsection{مقایسه سه روش}
	به طور کلی در شکل
	روند کاهش مقدار خطا روی داده‌های تست نشان داده شده است. مدلی که از طریق بهینه سازی
	\lr{SGD}
	آموزش دیده بود، سریعا به همگرایی رسید اما روش 
	\lr{BGD}
	با روندی کندتر، اما ثابت و پیشبینی پذیر به همگرایی میرسد. دیده می‌شود (شکل
	\ref{fig: loss_comparison}
	) که هر دو روش با کمترین خطای ممکن کمتر از 2.0 فاصله دارند.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{figs/loss_comparison}
		\caption{روند کاهش خطای تست در سه روش 
		\lr{SGD, SGD}
		و راه حل بسته
		}
		\label{fig: loss_comparison}
	\end{figure}
	یکی دیگر از مقایسه‌ها، تفاوت مقدار میانگین خطا تست نسبت به اندازه دیتاست می‌باشد. همانطور که در شکل
	 \ref{fig: dataset_comparison}
	دیده می‌شود، در ابتدا روش‌های جستجو مانند گرادیان کاهشی بهتر از راه حل بسته ظاهر شده اند.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{figs/dataset_size_compar}
		\caption{میانگین خطا تست به اندازه دیتاست}
		\label{fig: dataset_comparison}
	\end{figure}
	\subsection{اضافه کردن $L_2$ به راه حل بسته}
	با اضافه کردن ترم $L_2$ به تابع هزینه، به رابطه زیر برای به دست آوردن پارامتر $\theta$ می‌رسیم.
	\[ \theta = (X^TX + \lambda I)^{-1}X^T\vec{y} \]
	برای به دست آوردن مقدار مناسب $\lambda$، مقدار خطای تست پرای پارامترهای به دست آمده با مقادیر مختلف $\lambda$ در شکل 
	\ref{fig: lambda_comparison}
	دیده شده است.
	\begin{figure}[H]
		\centering
		\includegraphics[scale=0.5]{figs/diff_lambda}
		\caption{میانگین خطا تست برای مقادیر متفاوت $\lambda$ (کمترین مقدار خطای تست، با مقدار $\lambda = 10^{-4}$ به دست می‌اید)}
		\label{fig: lambda_comparison}
	\end{figure}
	
	
	
	
	
\end{document}